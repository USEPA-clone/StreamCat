{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from ..StreamCat_functions_gpu import bastards, dbf2DF, nhd_dict, make_all_cat_comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    all_comids = make_all_cat_comids(nhd, inputs)\n",
    "    print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "\n",
    "    def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = dbf2DF(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\")[[\"TOCOMID\", \"FROMCOMID\"]]\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = dbf2DF(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\")\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = cp.setdiff1d(cp.array(flow.FROMCOMID.values), cp.array(fls.COMID.values))\n",
    "        out = out[cp.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(cp.asnumpy(cp.setdiff1d(out, cp.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "        comids = cp.array(cats.index.values)\n",
    "        comids = cp.append(comids, cp.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in cp.asnumpy(comids)]\n",
    "        lengths = cp.array([len(u) for u in ups])\n",
    "        upstream = cp.hstack(ups).astype(cp.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        cp.savez_compressed(\n",
    "            f\"./accum_npy/accum_{zone}.npz\",\n",
    "            comids=cp.asnumpy(comids),\n",
    "            lengths=cp.asnumpy(lengths),\n",
    "            upstream=cp.asnumpy(upstream),\n",
    "        )\n",
    "\n",
    "    # Parallel processing\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with xarray implementation\n",
    "import xarray as xr\n",
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    all_comids = make_all_cat_comids(nhd, inputs)\n",
    "    print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    comids_list = []\n",
    "    lengths_list = []\n",
    "    upstream_list = []\n",
    "\n",
    "    def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = dbf2DF(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\")[[\"TOCOMID\", \"FROMCOMID\"]]\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = dbf2DF(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\")\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = cp.setdiff1d(cp.array(flow.FROMCOMID.values), cp.array(fls.COMID.values))\n",
    "        out = out[cp.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(cp.asnumpy(cp.setdiff1d(out, cp.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "        comids = cp.array(cats.index.values)\n",
    "        comids = cp.append(comids, cp.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in cp.asnumpy(comids)]\n",
    "        lengths = cp.array([len(u) for u in ups])\n",
    "        upstream = cp.hstack(ups).astype(cp.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        \n",
    "        # Append results to lists\n",
    "        comids_list.append(cp.asnumpy(comids))\n",
    "        lengths_list.append(cp.asnumpy(lengths))\n",
    "        upstream_list.append(cp.asnumpy(upstream))\n",
    "\n",
    "    # Parallel processing\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "\n",
    "    # Convert lists to Xarray DataArrays\n",
    "    comids_da = xr.DataArray(da.concatenate([da.from_array(arr) for arr in comids_list]), dims=['index'])\n",
    "    lengths_da = xr.DataArray(da.concatenate([da.from_array(arr) for arr in lengths_list]), dims=['index'])\n",
    "    upstream_da = xr.DataArray(da.concatenate([da.from_array(arr) for arr in upstream_list]), dims=['index'])\n",
    "\n",
    "    # Create a Dataset\n",
    "    ds = xr.Dataset({\n",
    "        'comids': comids_da,\n",
    "        'lengths': lengths_da,\n",
    "        'upstream': upstream_da\n",
    "    })\n",
    "\n",
    "    # Save the Dataset to a NetCDF file\n",
    "    ds.to_netcdf('accum_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    all_comids = make_all_cat_comids(nhd, inputs)\n",
    "    print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    comids_list = []\n",
    "    lengths_list = []\n",
    "    upstream_list = []\n",
    "\n",
    "    def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = dbf2DF(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\")[[\"TOCOMID\", \"FROMCOMID\"]]\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = dbf2DF(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\")\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = cp.setdiff1d(cp.array(flow.FROMCOMID.values), cp.array(fls.COMID.values))\n",
    "        out = out[cp.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(cp.asnumpy(cp.setdiff1d(out, cp.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "        comids = cp.array(cats.index.values)\n",
    "        comids = cp.append(comids, cp.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in cp.asnumpy(comids)]\n",
    "        lengths = cp.array([len(u) for u in ups])\n",
    "        upstream = cp.hstack(ups).astype(cp.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        \n",
    "        # Append results to lists\n",
    "        comids_list.append(cp.asnumpy(comids))\n",
    "        lengths_list.append(cp.asnumpy(lengths))\n",
    "        upstream_list.append(cp.asnumpy(upstream))\n",
    "\n",
    "    # Parallel processing\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "\n",
    "    # Convert lists to Dask arrays\n",
    "    comids_dask = da.concatenate([da.from_array(arr) for arr in comids_list])\n",
    "    lengths_dask = da.concatenate([da.from_array(arr) for arr in lengths_list])\n",
    "    upstream_dask = da.concatenate([da.from_array(arr) for arr in upstream_list])\n",
    "\n",
    "    # Save Dask arrays to a single file\n",
    "    da.to_zarr(comids_dask, 'comids.zarr', mode='w')\n",
    "    da.to_zarr(lengths_dask, 'lengths.zarr', mode='w')\n",
    "    da.to_zarr(upstream_dask, 'upstream.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_vpu = pd.read_csv(\"../config_tables/InterVPU.csv\")\n",
    "NHD_DIR = \"O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Resource/Physical/HYDROLOGY/NHDPlusV21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "makeNumpyVectors(inter_vpu, NHD_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
