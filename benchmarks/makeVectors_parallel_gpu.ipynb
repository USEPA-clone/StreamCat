{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import pyogrio\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from StreamCat_functions_gpu import bastards, dbf2DF, nhd_dict, make_all_cat_comids, gpu_bastards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  numpy\n",
      "[1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CUDA Device 0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cupy test\n",
    "xp = cp.get_array_module()\n",
    "print(\"Using: \", xp.__name__)\n",
    "x_gpu = cp.array([1, 2, 3])\n",
    "print(x_gpu)\n",
    "x_gpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = pyogrio.read_dataframe(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\", columns=[\"TOCOMID\", \"FROMCOMID\"], read_geometry=False, use_arrow=True)\n",
    "        flow.columns = flow.columns.str.upper()\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = pyogrio.read_dataframe(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\", read_geometry=False, use_arrow=True)\n",
    "        fls.columns = fls.columns.str.upper()\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = cp.setdiff1d(cp.array(flow.FROMCOMID.values), cp.array(fls.COMID.values))\n",
    "        out = out[cp.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(cp.asnumpy(cp.setdiff1d(out, cp.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = pyogrio.read_dataframe(f\"{pre}/NHDPlusCatchment/Catchment.dbf\", read_geometry=False, use_arrow=True)\n",
    "        cats.columns = cats.columns.str.upper()\n",
    "        cats = cats.set_index(\"FEATUREID\")\n",
    "        comids = cp.array(cats.index.values)\n",
    "        comids = cp.append(comids, cp.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(gpu_bastards(x, flow_dict))) for x in cp.asnumpy(comids)]\n",
    "        lengths = cp.array([len(u) for u in ups])\n",
    "        upstream = cp.hstack(ups).astype(cp.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        cp.savez_compressed(\n",
    "            f\"./accum_npy/accum_{zone}.npz\",\n",
    "            comids=cp.asnumpy(comids),\n",
    "            lengths=cp.asnumpy(lengths),\n",
    "            upstream=cp.asnumpy(upstream),\n",
    "        )\n",
    "\n",
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    del inputs['16']\n",
    "    del inputs['15']\n",
    "    print(len(inputs))\n",
    "    all_comids = cp.load('accum_npy/allCatCOMs.npz')['all_comids']\n",
    "    all_comids = set(all_comids.flatten())\n",
    "\n",
    "    # Parallel processing\n",
    "    print(\"Begining parallel execution:\")\n",
    "    start_time = time.time()\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "    end_time = time.time()\n",
    "    print(f\"Time elapsed to process all zones: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with xarray implementation\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    all_comids = make_all_cat_comids(nhd, inputs)\n",
    "    print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    comids_list = []\n",
    "    lengths_list = []\n",
    "    upstream_list = []\n",
    "\n",
    "    def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = dbf2DF(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\")[[\"TOCOMID\", \"FROMCOMID\"]]\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = dbf2DF(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\")\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = cp.setdiff1d(cp.array(flow.FROMCOMID.values), cp.array(fls.COMID.values))\n",
    "        out = out[cp.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(cp.asnumpy(cp.setdiff1d(out, cp.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "        comids = cp.array(cats.index.values)\n",
    "        comids = cp.append(comids, cp.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in cp.asnumpy(comids)]\n",
    "        lengths = cp.array([len(u) for u in ups])\n",
    "        upstream = cp.hstack(ups).astype(cp.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        \n",
    "        # Append results to lists\n",
    "        comids_list.append(cp.asnumpy(comids))\n",
    "        lengths_list.append(cp.asnumpy(lengths))\n",
    "        upstream_list.append(cp.asnumpy(upstream))\n",
    "\n",
    "    # Parallel processing\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "\n",
    "    # Convert lists to Xarray DataArrays\n",
    "    comids_da = xr.DataArray(da.concatenate([da.from_array(arr) for arr in comids_list]), dims=['index'])\n",
    "    lengths_da = xr.DataArray(da.concatenate([da.from_array(arr) for arr in lengths_list]), dims=['index'])\n",
    "    upstream_da = xr.DataArray(da.concatenate([da.from_array(arr) for arr in upstream_list]), dims=['index'])\n",
    "\n",
    "    # Create a Dataset\n",
    "    ds = xr.Dataset({\n",
    "        'comids': comids_da,\n",
    "        'lengths': lengths_da,\n",
    "        'upstream': upstream_da\n",
    "    })\n",
    "\n",
    "    # Save the Dataset to a NetCDF file\n",
    "    ds.to_netcdf('accum_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    all_comids = make_all_cat_comids(nhd, inputs)\n",
    "    print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    comids_list = []\n",
    "    lengths_list = []\n",
    "    upstream_list = []\n",
    "\n",
    "    def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = dbf2DF(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\")[[\"TOCOMID\", \"FROMCOMID\"]]\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = dbf2DF(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\")\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = cp.setdiff1d(cp.array(flow.FROMCOMID.values), cp.array(fls.COMID.values))\n",
    "        out = out[cp.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(cp.asnumpy(cp.setdiff1d(out, cp.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "        comids = cp.array(cats.index.values)\n",
    "        comids = cp.append(comids, cp.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in cp.asnumpy(comids)]\n",
    "        lengths = cp.array([len(u) for u in ups])\n",
    "        upstream = cp.hstack(ups).astype(cp.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        \n",
    "        # Append results to lists\n",
    "        comids_list.append(cp.asnumpy(comids))\n",
    "        lengths_list.append(cp.asnumpy(lengths))\n",
    "        upstream_list.append(cp.asnumpy(upstream))\n",
    "\n",
    "    # Parallel processing\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "\n",
    "    # Convert lists to Dask arrays\n",
    "    comids_dask = da.concatenate([da.from_array(arr) for arr in comids_list])\n",
    "    lengths_dask = da.concatenate([da.from_array(arr) for arr in lengths_list])\n",
    "    upstream_dask = da.concatenate([da.from_array(arr) for arr in upstream_list])\n",
    "\n",
    "    # Save Dask arrays to a single file\n",
    "    da.to_zarr(comids_dask, 'comids.zarr', mode='w')\n",
    "    da.to_zarr(lengths_dask, 'lengths.zarr', mode='w')\n",
    "    da.to_zarr(upstream_dask, 'upstream.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_vpu = pd.read_csv(\"../config_tables/InterVPU.csv\")\n",
    "NHD_DIR = \"O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Resource/Physical/HYDROLOGY/NHDPlusV21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CuPy failed to load nvrtc64_120_0.dll: FileNotFoundError: Could not find module 'nvrtc64_120_0.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mcupy_backends\\cuda\\_softlink.pyx:25\u001b[0m, in \u001b[0;36mcupy_backends.cuda._softlink.SoftLink.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\ctypes\\__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'nvrtc64_120_0.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmakeNumpyVectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minter_vpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNHD_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m, in \u001b[0;36mmakeNumpyVectors\u001b[1;34m(inter_tbl, nhd)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m     52\u001b[0m all_comids \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccum_npy/allCatCOMs.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_comids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 53\u001b[0m all_comids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mall_comids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Parallel processing\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBegining parallel execution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:776\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.flatten\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:795\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.flatten\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_manipulation.pyx:129\u001b[0m, in \u001b[0;36mcupy._core._routines_manipulation._ndarray_flatten\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_routines_manipulation.pyx:138\u001b[0m, in \u001b[0;36mcupy._core._routines_manipulation._ndarray_flatten_order_c\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:611\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:580\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_kernel.pyx:1375\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_kernel.pyx:1402\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc._get_ufunc_kernel\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_kernel.pyx:1082\u001b[0m, in \u001b[0;36mcupy._core._kernel._get_ufunc_kernel\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_kernel.pyx:94\u001b[0m, in \u001b[0;36mcupy._core._kernel._get_simple_elementwise_kernel\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\_kernel.pyx:82\u001b[0m, in \u001b[0;36mcupy._core._kernel._get_simple_elementwise_kernel_from_code\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy\\_core\\core.pyx:2254\u001b[0m, in \u001b[0;36mcupy._core.core.compile_with_cache\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\cuda\\compiler.py:484\u001b[0m, in \u001b[0;36m_compile_module_with_cache\u001b[1;34m(source, options, arch, cache_dir, extra_source, backend, enable_cooperative_groups, name_expressions, log_stream, jitify)\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile_with_cache_hip(\n\u001b[0;32m    481\u001b[0m         source, options, arch, cache_dir, extra_source, backend,\n\u001b[0;32m    482\u001b[0m         name_expressions, log_stream, cache_in_memory)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_with_cache_cuda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_cooperative_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_expressions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_in_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitify\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\cuda\\compiler.py:499\u001b[0m, in \u001b[0;36m_compile_with_cache_cuda\u001b[1;34m(source, options, arch, cache_dir, extra_source, backend, enable_cooperative_groups, name_expressions, log_stream, cache_in_memory, jitify)\u001b[0m\n\u001b[0;32m    497\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m get_cache_dir()\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     arch \u001b[38;5;241m=\u001b[39m \u001b[43m_get_arch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m options \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-ftz=true\u001b[39m\u001b[38;5;124m'\u001b[39m,)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_cooperative_groups:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# `cooperative_groups` requires relocatable device code.\u001b[39;00m\n",
      "File \u001b[1;32mcupy\\_util.pyx:64\u001b[0m, in \u001b[0;36mcupy._util.memoize.decorator.ret\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\cuda\\compiler.py:148\u001b[0m, in \u001b[0;36m_get_arch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;129m@_util\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(for_each_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_arch\u001b[39m():\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# See Supported Compile Options section of NVRTC User Guide for\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# the maximum value allowed for `--gpu-architecture`.\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m     nvrtc_max_compute_capability \u001b[38;5;241m=\u001b[39m \u001b[43m_get_max_compute_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     arch \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mDevice()\u001b[38;5;241m.\u001b[39mcompute_capability\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arch \u001b[38;5;129;01min\u001b[39;00m _tegra_archs:\n",
      "File \u001b[1;32mcupy\\_util.pyx:64\u001b[0m, in \u001b[0;36mcupy._util.memoize.decorator.ret\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\cuda\\compiler.py:126\u001b[0m, in \u001b[0;36m_get_max_compute_capability\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@_util\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_max_compute_capability\u001b[39m():\n\u001b[1;32m--> 126\u001b[0m     major, minor \u001b[38;5;241m=\u001b[39m \u001b[43m_get_nvrtc_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m11\u001b[39m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# CUDA 10.2\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         nvrtc_max_compute_capability \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m75\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\cuda\\compiler.py:115\u001b[0m, in \u001b[0;36m_get_nvrtc_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _nvrtc_version\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _nvrtc_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     _nvrtc_version \u001b[38;5;241m=\u001b[39m \u001b[43mnvrtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nvrtc_version\n",
      "File \u001b[1;32mcupy_backends\\cuda\\libs\\nvrtc.pyx:56\u001b[0m, in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.getVersion\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\libs\\nvrtc.pyx:57\u001b[0m, in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.getVersion\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\libs\\_cnvrtc.pxi:72\u001b[0m, in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.initialize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\libs\\_cnvrtc.pxi:76\u001b[0m, in \u001b[0;36mcupy_backends.cuda.libs.nvrtc._initialize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\libs\\_cnvrtc.pxi:143\u001b[0m, in \u001b[0;36mcupy_backends.cuda.libs.nvrtc._get_softlink\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mcupy_backends\\cuda\\_softlink.pyx:32\u001b[0m, in \u001b[0;36mcupy_backends.cuda._softlink.SoftLink.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CuPy failed to load nvrtc64_120_0.dll: FileNotFoundError: Could not find module 'nvrtc64_120_0.dll' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "makeNumpyVectors(inter_vpu, NHD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx.profiler import benchmark\n",
    "benchmark(makeNumpyVectors, (inter_vpu, NHD_DIR), n_repeat=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
