{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\numba\\core\\decorators.py:248: RuntimeWarning: forceobj is set for njit and is ignored\n",
      "  warnings.warn('forceobj is set for njit and is ignored', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from StreamCat_functions_gpu import bastards, dbf2DF, nhd_dict, make_all_cat_comids, numba_bastards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check PlusFlow.dbf and NHDFlowline.dbf for geometry values\n",
    "flow_path = \"O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Resource/Physical/HYDROLOGY/NHDPlusV21/NHDPlusMS/NHDPlus06/NHDPlusAttributes/PlusFlow.dbf\"\n",
    "flow = gpd.read_file(flow_path)[[\"TOCOMID\", \"FROMCOMID\"]] # dbf2DF(flow_path)\n",
    "flow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fls_path = \"O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Resource/Physical/HYDROLOGY/NHDPlusV21/NHDPlusMS/NHDPlus06/NHDSnapshot/Hydrography/NHDFlowline.dbf\"\n",
    "# could just do gpd.read_file(fls_path)\n",
    "fls = gpd.read_file(fls_path) # dbf2DF(fls_path)\n",
    "fls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_tbl = pd.read_csv(\"../config_tables/InterVPU.csv\")\n",
    "inter_tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comids = np.load('accum_npy/allCatCOMs.npz')['all_comids']\n",
    "all_comids = set(all_comids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = \"O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Resource/Physical/HYDROLOGY/NHDPlusV21/NHDPlusMS/NHDPlus06\"\n",
    "zone = '06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "out = np.setdiff1d(flow.FROMCOMID.values, fls.COMID.values)\n",
    "out = out[np.nonzero(out)]\n",
    "flow = flow[~flow.FROMCOMID.isin(np.setdiff1d(out, inter_tbl.thruCOMIDs.values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_dict = defaultdict(list)\n",
    "for _, row in flow.iterrows():\n",
    "    flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "\n",
    "for interLine in inter_tbl.values:\n",
    "    if interLine[6] > 0 and interLine[2] == zone:\n",
    "        flow_dict[int(interLine[6])].append(int(interLine[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_vpus = inter_tbl.loc[\n",
    "    (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "].thruCOMIDs.values\n",
    "cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "comids = cats.index.values\n",
    "comids = np.append(comids, out_of_vpus)\n",
    "comids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "children = [bastards(x, flow_dict) for x in comids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#children = [bastards(x, flow_dict) for x in comids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core import types \n",
    "from numba.typed import Dict \n",
    "d = Dict.empty(\n",
    "    key_type = types.int32,\n",
    "    value_type = types.int32[:]\n",
    ")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_dict_standard = dict(flow_dict)\n",
    "# flow_dict_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# numba_children = [numba_bastards(x, flow_dict_standard) for x in comids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_list = [i for sublist in children for i in sublist]\n",
    "# print(len(flattened_list))\n",
    "# flattened_children = np.array(flattened_list)\n",
    "# flattened_children.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ups = np.intersect1d(all_comids, flattened_children)\n",
    "# ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in comids]\n",
    "lengths = np.array([len(u) for u in ups])\n",
    "upstream = np.hstack(ups).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ups) == len(lengths) == len(comids)\n",
    "np.savez_compressed(\n",
    "    f\"./accum_npy/accum_{zone}_speed_test.npz\",\n",
    "    comids=comids,\n",
    "    lengths=lengths,\n",
    "    upstream=upstream,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _standard_make_all_cat_comids(nhd, inputs):\n",
    "    all_comids = np.array([], dtype=np.int32)\n",
    "    start_time = time.time()\n",
    "    for zone, hr in inputs.items():\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\")\n",
    "        all_comids = np.append(all_comids, cats.FEATUREID.values.astype(int))\n",
    "    end_time = time.time()\n",
    "    print(f\"Time elapsed in standard function: {end_time - start_time} seconds\")\n",
    "    # np.savez_compressed(\"./accum_npy/allCatCOMs.npz\", all_comids=all_comids)\n",
    "    return all_comids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyogrio\n",
    "def make_zone_cat_comids(nhd, zone, hr):\n",
    "    path = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}/NHDPlusCatchment/Catchment.dbf\"\n",
    "    cats = pyogrio.read_dataframe(path, columns=['FEATUREID'], read_geometry=False, use_arrow=True)\n",
    "    \n",
    "    return cats.values.astype(int)\n",
    "\n",
    "def _pyogrio_make_all_cat_comids(nhd, inputs):\n",
    "    print(\"Making allFLOWCOMs numpy file\")\n",
    "    start_time = time.time()\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(make_zone_cat_comids)(nhd, zone, hr) for zone, hr in inputs.items()\n",
    "    )\n",
    "    print(results)\n",
    "    end_time = time.time()\n",
    "    all_comids = np.concatenate(results)\n",
    "    print(f\"Time elapsed in parallel pyogrio function: {end_time - start_time} seconds\")\n",
    "    print(all_comids.shape)\n",
    "    return all_comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "    #print(zone, end=\", \", flush=True)\n",
    "    pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "    flow = pyogrio.read_dataframe(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\", columns=[\"TOCOMID\", \"FROMCOMID\"], read_geometry=False, use_arrow=True)\n",
    "    flow.columns = flow.columns.str.upper()\n",
    "    flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "    fls = pyogrio.read_dataframe(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\", read_geometry=False, use_arrow=True)\n",
    "    fls.columns = fls.columns.str.upper()\n",
    "    coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "    flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "    flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "    out = np.setdiff1d(flow.FROMCOMID.values, fls.COMID.values)\n",
    "    out = out[np.nonzero(out)]\n",
    "    flow = flow[~flow.FROMCOMID.isin(np.setdiff1d(out, inter_tbl.thruCOMIDs.values))]\n",
    "    \n",
    "    flow_dict = defaultdict(list)\n",
    "    for _, row in flow.iterrows():\n",
    "        flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "    \n",
    "    for interLine in inter_tbl.values:\n",
    "        if interLine[6] > 0 and interLine[2] == zone:\n",
    "            flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "    \n",
    "    out_of_vpus = inter_tbl.loc[\n",
    "        (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "    ].thruCOMIDs.values\n",
    "    cats = pyogrio.read_dataframe(f\"{pre}/NHDPlusCatchment/Catchment.dbf\", read_geometry=False, use_arrow=True)\n",
    "    cats.columns = cats.columns.str.upper()\n",
    "    cats = cats.set_index(\"FEATUREID\")\n",
    "    comids = cats.index.values\n",
    "    comids = np.append(comids, out_of_vpus)\n",
    "    \n",
    "    ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in comids]\n",
    "    lengths = np.array([len(u) for u in ups])\n",
    "    upstream = np.hstack(ups).astype(np.int32)\n",
    "    \n",
    "    assert len(ups) == len(lengths) == len(comids)\n",
    "    np.savez_compressed(\n",
    "        f\"./accum_npy/accum_{zone}_speed_test2.npz\",\n",
    "        comids=comids,\n",
    "        lengths=lengths,\n",
    "        upstream=upstream,\n",
    "    )\n",
    "\n",
    "def makeNumpyVectors(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    del inputs['16']\n",
    "    del inputs['15']\n",
    "    print(len(inputs))\n",
    "    #print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "    all_comids_test = _pyogrio_make_all_cat_comids(nhd, inputs)\n",
    "    all_comids_test = set(all_comids_test.flatten())\n",
    "    #all_comids_test1 = _standard_make_all_cat_comids(nhd, inputs)  #make_all_cat_comids(nhd, inputs) #TODO this function take ~80 minutes\n",
    "    \n",
    "    all_comids = np.load('accum_npy/allCatCOMs.npz')['all_comids']\n",
    "    all_comids = set(all_comids)\n",
    "    if all_comids == all_comids_test:\n",
    "        print(\"Test parallel function produces equivalent results\")\n",
    "    else:\n",
    "        print(\"NOT EQUIVALENT\")\n",
    "\n",
    "    # Parallel processing\n",
    "    print(\"Begining parallel execution:\")\n",
    "    start_time = time.time()\n",
    "    Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "    end_time = time.time()\n",
    "    print(f\"Time elapsed to process all zones: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thruCOMIDs</th>\n",
       "      <th>FromZone</th>\n",
       "      <th>ToZone</th>\n",
       "      <th>AdjustComs</th>\n",
       "      <th>toCOMIDs</th>\n",
       "      <th>DropCOMID</th>\n",
       "      <th>UpCOMadd</th>\n",
       "      <th>removeCOMs</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18267741</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20734041</td>\n",
       "      <td>24719331</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20734037</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10466473</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1861888</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15714785</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1862004</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1862014</td>\n",
       "      <td>06</td>\n",
       "      <td>05</td>\n",
       "      <td>1862004</td>\n",
       "      <td>0</td>\n",
       "      <td>1862014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   thruCOMIDs FromZone ToZone  AdjustComs  toCOMIDs  DropCOMID  UpCOMadd  \\\n",
       "0    18267741       14     15           0         0          0  20734041   \n",
       "1    20734037       14     15           0         0          0         0   \n",
       "2     1861888       06     05           0         0          0         0   \n",
       "3     1862004       06     05           0         0          0         0   \n",
       "4     1862014       06     05     1862004         0    1862014         0   \n",
       "\n",
       "   removeCOMs comments  \n",
       "0    24719331      NaN  \n",
       "1    10466473      NaN  \n",
       "2    15714785      NaN  \n",
       "3           0      NaN  \n",
       "4           0      NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_vpu = pd.read_csv(\"../config_tables/InterVPU.csv\")\n",
    "NHD_DIR = \"O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Resource/Physical/HYDROLOGY/NHDPlusV21\"\n",
    "inter_vpu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Making allFLOWCOMs numpy file\n",
      "[array([[19752703],\n",
      "       [19751623],\n",
      "       [19752127],\n",
      "       ...,\n",
      "       [ 1862750],\n",
      "       [ 1861696],\n",
      "       [ 1863344]]), array([[9049935],\n",
      "       [9049143],\n",
      "       [9049659],\n",
      "       ...,\n",
      "       [ -70117],\n",
      "       [ -70118],\n",
      "       [ -70119]]), array([[3026444],\n",
      "       [3023730],\n",
      "       [3024294],\n",
      "       ...,\n",
      "       [-129224],\n",
      "       [-129225],\n",
      "       [-129226]]), array([[22106719],\n",
      "       [22106165],\n",
      "       [22104741],\n",
      "       ...,\n",
      "       [ -177033],\n",
      "       [ -177034],\n",
      "       [ -177035]]), array([[4972663],\n",
      "       [4972685],\n",
      "       [4973375],\n",
      "       ...,\n",
      "       [-203749],\n",
      "       [-203750],\n",
      "       [-203751]]), array([[7621924],\n",
      "       [7623326],\n",
      "       [7621692],\n",
      "       ...,\n",
      "       [-229510],\n",
      "       [-229511],\n",
      "       [-229512]]), array([[1234109],\n",
      "       [1234167],\n",
      "       [1234653],\n",
      "       ...,\n",
      "       [-357040],\n",
      "       [-357041],\n",
      "       [-357042]]), array([[ 718276],\n",
      "       [ 718808],\n",
      "       [ 718792],\n",
      "       ...,\n",
      "       [4600101],\n",
      "       [4599761],\n",
      "       [4599755]]), array([[24324012],\n",
      "       [24324160],\n",
      "       [24324144],\n",
      "       ...,\n",
      "       [23989745],\n",
      "       [24255303],\n",
      "       [23021612]]), array([[17905797],\n",
      "       [17906829],\n",
      "       [17906917],\n",
      "       ...,\n",
      "       [ -504119],\n",
      "       [ -504120],\n",
      "       [ -504121]]), array([[5279082],\n",
      "       [5276196],\n",
      "       [5279828],\n",
      "       ...,\n",
      "       [-307113],\n",
      "       [-307114],\n",
      "       [-307115]]), array([[6668463],\n",
      "       [6668581],\n",
      "       [6667851],\n",
      "       ...,\n",
      "       [-276768],\n",
      "       [-276769],\n",
      "       [-300123]]), array([[22220519],\n",
      "       [22220973],\n",
      "       [22221761],\n",
      "       ...,\n",
      "       [10056162],\n",
      "       [10055328],\n",
      "       [10055318]]), array([[ 15787092],\n",
      "       [ 15786760],\n",
      "       [ 15787470],\n",
      "       ...,\n",
      "       [938090308],\n",
      "       [ 11083553],\n",
      "       [ 14320631]]), array([[4798362],\n",
      "       [4795168],\n",
      "       [4794584],\n",
      "       ...,\n",
      "       [  -6032],\n",
      "       [  -6033],\n",
      "       [  -6034]]), array([[ 10317646],\n",
      "       [ 10319798],\n",
      "       [ 10319854],\n",
      "       ...,\n",
      "       [ 18076876],\n",
      "       [933120085],\n",
      "       [  -450549]]), array([[1058079],\n",
      "       [6336396],\n",
      "       [6335562],\n",
      "       ...,\n",
      "       [-504162],\n",
      "       [-504163],\n",
      "       [-504164]]), array([[  8623983],\n",
      "       [  8624251],\n",
      "       [  8624757],\n",
      "       ...,\n",
      "       [  6321799],\n",
      "       [  6318309],\n",
      "       [166741921]]), array([[22226474],\n",
      "       [22227584],\n",
      "       [22227356],\n",
      "       ...,\n",
      "       [ -258275],\n",
      "       [ -258276],\n",
      "       [ -258277]])]\n",
      "Time elapsed in parallel pyogrio function: 65.9832398891449 seconds\n",
      "(2452068, 1)\n",
      "Test parallel function produces equivalent results\n",
      "Begining parallel execution:\n",
      "Time elapsed to process all zones: 1052.3388922214508 seconds\n"
     ]
    }
   ],
   "source": [
    "makeNumpyVectors(inter_vpu, NHD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# could use dask arrays instead of writing to files\n",
    "# add to above function\n",
    "import dask.array as da\n",
    "def makeNumpyVectors_dask(inter_tbl, nhd):\n",
    "    os.makedirs(\"accum_npy\", exist_ok=True)\n",
    "    inputs = nhd_dict(nhd)\n",
    "    inputs.pop('16')\n",
    "    print(inputs)\n",
    "    all_comids = make_all_cat_comids(nhd, inputs)\n",
    "    print(\"Making numpy files in zone...\", end=\"\", flush=True)\n",
    "\n",
    "    # Initialize empty lists to store results\n",
    "    comids_list = []\n",
    "    lengths_list = []\n",
    "    upstream_list = []\n",
    "\n",
    "    def process_zone(zone, hr, nhd, inter_tbl, all_comids):\n",
    "        print(zone, end=\", \", flush=True)\n",
    "        pre = f\"{nhd}/NHDPlus{hr}/NHDPlus{zone}\"\n",
    "        flow = dbf2DF(f\"{pre}/NHDPlusAttributes/PlusFlow.dbf\")[[\"TOCOMID\", \"FROMCOMID\"]]\n",
    "        flow = flow[(flow.TOCOMID != 0) & (flow.FROMCOMID != 0)]\n",
    "        fls = dbf2DF(f\"{pre}/NHDSnapshot/Hydrography/NHDFlowline.dbf\")\n",
    "        coastfl = fls.COMID[fls.FTYPE == \"Coastline\"]\n",
    "        flow = flow[~flow.FROMCOMID.isin(coastfl.values)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(inter_tbl.removeCOMs)]\n",
    "        \n",
    "        out = np.setdiff1d(np.array(flow.FROMCOMID.values), np.array(fls.COMID.values))\n",
    "        out = out[np.nonzero(out)]\n",
    "        flow = flow[~flow.FROMCOMID.isin(np.asnumpy(np.setdiff1d(out, np.array(inter_tbl.thruCOMIDs.values))))]\n",
    "\n",
    "        flow_dict = defaultdict(list)\n",
    "        for _, row in flow.iterrows():\n",
    "            flow_dict[row.TOCOMID].append(row.FROMCOMID)\n",
    "        \n",
    "        for interLine in inter_tbl.values:\n",
    "            if interLine[6] > 0 and interLine[2] == zone:\n",
    "                flow_dict[int(interLine[6])].append(int(interLine[0]))\n",
    "        \n",
    "        out_of_vpus = inter_tbl.loc[\n",
    "            (inter_tbl.ToZone == zone) & (inter_tbl.DropCOMID == 0)\n",
    "        ].thruCOMIDs.values\n",
    "        cats = dbf2DF(f\"{pre}/NHDPlusCatchment/Catchment.dbf\").set_index(\"FEATUREID\")\n",
    "        comids = np.array(cats.index.values)\n",
    "        comids = np.append(comids, np.array(out_of_vpus))\n",
    "        \n",
    "        ups = [list(all_comids.intersection(bastards(x, flow_dict))) for x in np.asnumpy(comids)]\n",
    "        lengths = np.array([len(u) for u in ups])\n",
    "        upstream = np.hstack(ups).astype(np.int32)\n",
    "        \n",
    "        assert len(ups) == len(lengths) == len(comids)\n",
    "        \n",
    "        # Append results to lists\n",
    "        comids_list.append(np.asnumpy(comids))\n",
    "        lengths_list.append(np.asnumpy(lengths))\n",
    "        upstream_list.append(np.asnumpy(upstream))\n",
    "\n",
    "    # Parallel processing\n",
    "    retults = Parallel(n_jobs=-1)(delayed(process_zone)(zone, hr, nhd, inter_tbl, all_comids) for zone, hr in inputs.items())\n",
    "\n",
    "    # Convert lists to Dask arrays\n",
    "    comids_dask = da.concatenate([da.from_array(arr) for arr in comids_list])\n",
    "    lengths_dask = da.concatenate([da.from_array(arr) for arr in lengths_list])\n",
    "    upstream_dask = da.concatenate([da.from_array(arr) for arr in upstream_list])\n",
    "\n",
    "    # Save Dask arrays to a single file\n",
    "    da.to_zarr(comids_dask, 'comids.zarr', mode='w')\n",
    "    da.to_zarr(lengths_dask, 'lengths.zarr', mode='w')\n",
    "    da.to_zarr(upstream_dask, 'upstream.zarr', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
